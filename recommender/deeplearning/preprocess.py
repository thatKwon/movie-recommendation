"""
ë°ì´í„° ì „ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸
- NDJSON íŒŒì¼ì„ ì½ì–´ì„œ í•™ìŠµìš© ë°ì´í„°ë¡œ ë³€í™˜
- User-Item ìƒí˜¸ì‘ìš© ê·¸ë˜í”„ ìƒì„±
- Train/Valid/Test ë¶„ë¦¬
"""

import json
import numpy as np
import pandas as pd
from pathlib import Path
from collections import defaultdict
from sklearn.model_selection import train_test_split
import pickle
from tqdm import tqdm

class DataPreprocessor:
    def __init__(self, data_dir=".", output_dir="data/processed"):
        self.data_dir = Path(data_dir)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.movies = []
        self.peoples = {}
        self.ratings = []
        
    def load_ndjson(self, filename):
        """NDJSON íŒŒì¼ ë¡œë“œ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )"""
        file_path = self.data_dir / filename
        
        print(f"Loading {filename}...")
        
        # RatingsëŠ” pandasë¡œ ì§ì ‘ ë¡œë“œ (ë” ë¹ ë¥´ê³  ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
        if filename == 'ratings.ndjson':
            import pandas as pd
            # chunksizeë¡œ ë‚˜ëˆ ì„œ ì½ê¸°
            chunks = []
            for chunk in tqdm(pd.read_json(file_path, lines=True, chunksize=100000)):
                chunks.append(chunk)
            df = pd.concat(chunks, ignore_index=True)
            return df.to_dict('records')
        
        # Moviesì™€ PeoplesëŠ” ê¸°ì¡´ ë°©ì‹
        data = []
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in tqdm(f):
                try:
                    data.append(json.loads(line))
                except:
                    pass  # ì†ìƒëœ ë¼ì¸ ìŠ¤í‚µ
        
        return data
    
    def load_all_data(self):
        """ëª¨ë“  ë°ì´í„° ë¡œë“œ"""
        print("=" * 50)
        print("Step 1: Loading Raw Data")
        print("=" * 50)
        
        # Movies
        self.movies = self.load_ndjson('movies.ndjson')
        print(f"âœ“ Loaded {len(self.movies):,} movies")
        
        # Peoples
        peoples_list = self.load_ndjson('peoples.ndjson')
        self.peoples = {p['_id']: p for p in peoples_list}
        print(f"âœ“ Loaded {len(self.peoples):,} people")
        
        # Ratings
        self.ratings = self.load_ndjson('ratings.ndjson')
        print(f"âœ“ Loaded {len(self.ratings):,} ratings")
    
    def filter_data(self, min_user_interactions=5, min_movie_interactions=3, min_rating_threshold=7):
        """
        í¬ì†Œí•œ ë°ì´í„° í•„í„°ë§ + Implicit Feedback
        - Implicit feedback: Rating 7-10 positiveë¡œ ê°„ì£¼ (ë” ë§ì€ ë°ì´í„°)
        - í‰ì ì´ ë„ˆë¬´ ì ì€ ì‚¬ìš©ì/ì˜í™” ì œê±°
        - ë¹„ì •ìƒ ì‚¬ìš©ì ì œê±° (Rating 10ì„ ê³¼ë„í•˜ê²Œ ì£¼ëŠ” ì‚¬ìš©ì)
        """
        print("\n" + "=" * 50)
        print("Step 2: Implicit Feedback Filtering (Rating 7-10)")
        print("=" * 50)
        
        df = pd.DataFrame(self.ratings)
        print(f"Original: {len(df):,} ratings")
        
        # Rating 0 ì œê±°
        df = df[df['rate'] > 0]
        
        # Rating ë¶„í¬ ì¶œë ¥
        if 'rate' in df.columns:
            print(f"\nRating ë¶„í¬:")
            rating_dist = df['rate'].value_counts().sort_index()
            for rating, count in rating_dist.items():
                pct = 100 * count / len(df)
                marker = "âœ“" if rating >= min_rating_threshold else "âœ—"
                print(f"  {marker} Rating {rating:2d}: {count:7,} ({pct:5.2f}%)")
            
            # Threshold ì ìš© (Implicit Feedback: 8-10ë§Œ positive)
            df = df[df['rate'] >= min_rating_threshold]
            print(f"\nâœ“ After rating threshold (>= {min_rating_threshold}): {len(df):,} ratings ({100*len(df)/len(rating_dist):.1f}%)")
        
        # ë¹„ì •ìƒ ì‚¬ìš©ì ì œê±° (Rating 10ì„ 70% ì´ìƒ ì£¼ëŠ” ì‚¬ìš©ì)
        print(f"\nâ­ Removing abnormal users...")
        user_stats = df.groupby('user').agg({
            'rate': ['count', lambda x: (x == 10).sum()]
        })
        user_stats.columns = ['total', 'rating_10_count']
        user_stats['rating_10_ratio'] = user_stats['rating_10_count'] / user_stats['total']
        
        abnormal_users = user_stats[user_stats['rating_10_ratio'] > 0.7].index
        print(f"  Found {len(abnormal_users):,} abnormal users (>70% Rating 10)")
        
        df = df[~df['user'].isin(abnormal_users)]
        print(f"âœ“ After removing: {len(df):,} ratings")
        
        # ë°˜ë³µì ìœ¼ë¡œ í•„í„°ë§
        for iteration in range(5):
            # User í•„í„°ë§
            user_counts = df['user'].value_counts()
            valid_users = user_counts[user_counts >= min_user_interactions].index
            df = df[df['user'].isin(valid_users)]
            
            # Movie í•„í„°ë§
            movie_counts = df['movie'].value_counts()
            valid_movies = movie_counts[movie_counts >= min_movie_interactions].index
            df = df[df['movie'].isin(valid_movies)]
            
            print(f"Iteration {iteration + 1}: {len(df):,} ratings, "
                  f"{df['user'].nunique():,} users, {df['movie'].nunique():,} movies")
        
        self.ratings_filtered = df
        return df
    
    def create_mappings(self, df):
        """
        User/Movie IDë¥¼ ì—°ì†ëœ ì¸ë±ìŠ¤ë¡œ ë§¤í•‘
        """
        print("\n" + "=" * 50)
        print("Step 3: Creating ID Mappings")
        print("=" * 50)
        
        # User ë§¤í•‘
        unique_users = sorted(df['user'].unique())
        self.user2idx = {user: idx for idx, user in enumerate(unique_users)}
        self.idx2user = {idx: user for user, idx in self.user2idx.items()}
        
        # Movie ë§¤í•‘
        unique_movies = sorted(df['movie'].unique())
        self.movie2idx = {movie: idx for idx, movie in enumerate(unique_movies)}
        self.idx2movie = {idx: movie for movie, idx in self.movie2idx.items()}
        
        # ì˜í™” ì •ë³´ ë”•ì…”ë„ˆë¦¬
        self.movie_dict = {m['_id']: m for m in self.movies}
        
        print(f"âœ“ Users: {len(self.user2idx):,}")
        print(f"âœ“ Movies: {len(self.movie2idx):,}")
        
        # ë§¤í•‘ëœ ì¸ë±ìŠ¤ ì¶”ê°€
        df['user_idx'] = df['user'].map(self.user2idx)
        df['movie_idx'] = df['movie'].map(self.movie2idx)
        
        return df
    
    def extract_movie_features(self):
        """
        ì˜í™” ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
        """
        print("\n" + "=" * 50)
        print("Step 4: Extracting Movie Features")
        print("=" * 50)
        
        # ì¥ë¥´ ëª©ë¡ ìƒì„±
        all_genres = set()
        for movie in self.movies:
            if movie['_id'] in self.movie2idx:
                genres_str = movie.get('genres', [])
                if genres_str:
                    # "['ë“œë¼ë§ˆ', 'ë©œë¡œ/ë¡œë§¨ìŠ¤']" â†’ ['ë“œë¼ë§ˆ', 'ë©œë¡œ/ë¡œë§¨ìŠ¤']
                    try:
                        genres = eval(', '.join(genres_str))
                        all_genres.update(genres)
                    except:
                        pass
        
        self.all_genres = sorted(all_genres)
        self.genre2idx = {g: idx for idx, g in enumerate(self.all_genres)}
        
        print(f"âœ“ Found {len(self.all_genres)} genres")
        
        # ì˜í™”ë³„ íŠ¹ì„± ë²¡í„° ìƒì„±
        movie_features = []
        for idx in range(len(self.movie2idx)):
            movie_id = self.idx2movie[idx]
            movie = self.movie_dict.get(movie_id, {})
            
            # ì¥ë¥´ ì›-í•« ì¸ì½”ë”©
            genre_vector = np.zeros(len(self.all_genres))
            genres_str = movie.get('genres', [])
            if genres_str:
                try:
                    genres = eval(', '.join(genres_str))
                    for genre in genres:
                        if genre in self.genre2idx:
                            genre_vector[self.genre2idx[genre]] = 1
                except:
                    pass
            
            # ì—°ë„ ì •ê·œí™”
            year = movie.get('year', 2000)
            year_norm = (year - 1900) / 125.0 if year else 0.5
            
            # ì¶œì—°ì§„ ìˆ˜
            cast_count = len(movie.get('main_cast_people_ids', []))
            cast_norm = min(cast_count / 10.0, 1.0)
            
            # íŠ¹ì„± ê²°í•©
            features = np.concatenate([
                genre_vector,
                [year_norm, cast_norm]
            ])
            
            movie_features.append(features)
        
        self.movie_features = np.array(movie_features, dtype=np.float32)
        print(f"âœ“ Movie feature shape: {self.movie_features.shape}")
        
        return self.movie_features
    
    def split_data(self, df, test_ratio=0.15, valid_ratio=0.15):
        """
        Train/Valid/Test ë¶„ë¦¬ (Leave-One-Out - ë…¼ë¬¸ ë°©ì‹)
        
        **Leave-One-Out (ë…¼ë¬¸ê³¼ ë™ì¼)**:
        - ê° ì‚¬ìš©ìì˜ ë§ˆì§€ë§‰ ìƒí˜¸ì‘ìš©ì„ Testë¡œ ì‚¬ìš©
        - ë§ˆì§€ë§‰ì—ì„œ 2ë²ˆì§¸ ìƒí˜¸ì‘ìš©ì„ Validë¡œ ì‚¬ìš©
        - ë‚˜ë¨¸ì§€ë¥¼ Trainìœ¼ë¡œ ì‚¬ìš©
        - ì‹œê°„ ìˆœì„œ ê³ ë ¤ (ì‹¤ì œ ì¶”ì²œ í™˜ê²½ê³¼ ë™ì¼)
        - ë” ì–´ë µì§€ë§Œ í˜„ì‹¤ì ì¸ í‰ê°€
        
        Reference: 
        "We obtained the last interaction for every user" - ë…¼ë¬¸ Section 4.1.1
        """
        print("\n" + "=" * 50)
        print("Step 5: Splitting Data (LEAVE-ONE-OUT - Paper Method)")
        print("=" * 50)
        
        # ì‹œê°„ìˆœ ì •ë ¬ (ì¤‘ìš”!)
        if 'time' in df.columns:
            print("âœ“ Sorting by timestamp...")
            df = df.sort_values(['user_idx', 'time']).reset_index(drop=True)
        else:
            print("âš ï¸  No timestamp found, using original order")
            df = df.sort_values(['user_idx']).reset_index(drop=True)
        
        print(f"ì „ì²´ ë°ì´í„°: {len(df):,} ratings")
        
        # ê° ì‚¬ìš©ìë³„ ìƒí˜¸ì‘ìš© ìˆ˜ í™•ì¸
        user_counts = df.groupby('user_idx').size()
        print(f"\nì‚¬ìš©ìë‹¹ í‰ê·  ìƒí˜¸ì‘ìš©: {user_counts.mean():.1f}")
        print(f"ìµœì†Œ ìƒí˜¸ì‘ìš©: {user_counts.min()}")
        print(f"ìµœëŒ€ ìƒí˜¸ì‘ìš©: {user_counts.max()}")
        
        # Leave-One-Out Split
        # 1. Test: ê° ì‚¬ìš©ìì˜ ë§ˆì§€ë§‰ ìƒí˜¸ì‘ìš©
        test_indices = df.groupby('user_idx', sort=False).tail(1).index
        remaining_df = df.drop(test_indices)
        
        # 2. Valid: ê° ì‚¬ìš©ìì˜ ë§ˆì§€ë§‰ì—ì„œ 2ë²ˆì§¸ ìƒí˜¸ì‘ìš© (ë‚¨ì€ ê²ƒ ì¤‘ì—ì„œ)
        valid_indices = remaining_df.groupby('user_idx', sort=False).tail(1).index
        
        # 3. Train: ë‚˜ë¨¸ì§€
        train_indices = remaining_df.index.difference(valid_indices)
        
        train_df = df.loc[train_indices].copy()
        valid_df = df.loc[valid_indices].copy()
        test_df = df.loc[test_indices].copy()
        
        # ìƒí˜¸ì‘ìš© ìˆ˜ê°€ ë¶€ì¡±í•œ ì‚¬ìš©ì ì²˜ë¦¬
        # (2ê°œ ì´í•˜ì¸ ê²½ìš° validê°€ ë¹„ì–´ìˆì„ ìˆ˜ ìˆìŒ)
        users_with_valid = valid_df['user_idx'].nunique()
        users_with_test = test_df['user_idx'].nunique()
        total_users = df['user_idx'].nunique()
        
        print(f"\nâœ… Leave-One-Out Split ì™„ë£Œ:")
        print(f"  Train: {len(train_df):,} ratings ({100*len(train_df)/len(df):.1f}%)")
        print(f"  Valid: {len(valid_df):,} ratings ({100*len(valid_df)/len(df):.1f}%)")
        print(f"  Test:  {len(test_df):,} ratings ({100*len(test_df)/len(df):.1f}%)")
        print(f"  Total: {len(df):,} ratings")
        
        # ì‚¬ìš©ì/ì˜í™” ì»¤ë²„ë¦¬ì§€ í™•ì¸
        print(f"\nì‚¬ìš©ì ì»¤ë²„ë¦¬ì§€:")
        print(f"  Train: {train_df['user_idx'].nunique():,} users")
        print(f"  Valid: {valid_df['user_idx'].nunique():,} users ({100*users_with_valid/total_users:.1f}%)")
        print(f"  Test:  {test_df['user_idx'].nunique():,} users ({100*users_with_test/total_users:.1f}%)")
        
        print(f"\nì˜í™” ì»¤ë²„ë¦¬ì§€:")
        print(f"  Train: {train_df['movie_idx'].nunique():,} movies")
        print(f"  Valid: {valid_df['movie_idx'].nunique():,} movies")
        print(f"  Test:  {test_df['movie_idx'].nunique():,} movies")
        
        # ì‹œê°„ ìˆœì„œ ê²€ì¦
        if 'time' in df.columns:
            print(f"\nì‹œê°„ ìˆœì„œ ê²€ì¦:")
            print(f"  Train ì‹œê°„ ë²”ìœ„: {train_df['time'].min()} ~ {train_df['time'].max()}")
            print(f"  Valid ì‹œê°„ ë²”ìœ„: {valid_df['time'].min()} ~ {valid_df['time'].max()}")
            print(f"  Test  ì‹œê°„ ë²”ìœ„: {test_df['time'].min()} ~ {test_df['time'].max()}")
            
            # ê° ì‚¬ìš©ìë³„ë¡œ train < valid < testì¸ì§€ í™•ì¸
            print(f"\n  âœ“ Leave-One-Out: ê° ì‚¬ìš©ìë³„ë¡œ ì‹œê°„ ìˆœì„œ ë³´ì¥ë¨")
        
        print(f"\nâœ… Leave-One-Out ì™„ë£Œ (ë…¼ë¬¸ ë°©ì‹, ì‹¤ì œ ì¶”ì²œ í™˜ê²½ê³¼ ë™ì¼)")
        print(f"âš ï¸  ì£¼ì˜: Random splitë³´ë‹¤ ì–´ë ¤ìš´ í‰ê°€ (NDCGê°€ ë‚®ê²Œ ë‚˜ì˜¬ ìˆ˜ ìˆìŒ)")
        
        return train_df, valid_df, test_df
    
    def save_processed_data(self, train_df, valid_df, test_df, min_rating_threshold):
        """
        ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥
        """
        print("\n" + "=" * 50)
        print("Step 6: Saving Processed Data")
        print("=" * 50)
        
        # DataFrames ì €ì¥ (pklê³¼ csv ë‘˜ ë‹¤)
        train_df.to_pickle(self.output_dir / 'train.pkl')
        valid_df.to_pickle(self.output_dir / 'valid.pkl')
        test_df.to_pickle(self.output_dir / 'test.pkl')
        
        train_df.to_csv(self.output_dir / 'train.csv', index=False)
        valid_df.to_csv(self.output_dir / 'valid.csv', index=False)
        test_df.to_csv(self.output_dir / 'test.csv', index=False)
        
        # ì „ì²´ ì˜í™” ë§¤í•‘ (í‰ê°€ìš© - Rating â‰¥ threshold ì˜í™”ë§Œ í¬í•¨)
        # âš ï¸  í‰ì ì´ ìˆëŠ” ì˜í™”ë§Œ ì¶”ê°€ (ë©”íƒ€ë°ì´í„°ë§Œ ìˆëŠ” ì˜í™” ì œì™¸)
        all_movies_for_eval = {}
        
        # ratings.ndjsonì—ì„œ ì‹¤ì œ í‰ì ì´ ìˆëŠ” ì˜í™” ID ì¶”ì¶œ
        all_rated_movies = set()
        for rating in self.ratings:
            all_rated_movies.add(rating['movie'])
        
        print(f"\nğŸ“Š Movie Statistics:")
        print(f"  - Movies in metadata (movies.ndjson): {len(self.movies):,}")
        print(f"  - Movies with ratings: {len(all_rated_movies):,}")
        print(f"  - Movies without ratings: {len(self.movies) - len(all_rated_movies):,}")
        
        # í‰ì ì´ ìˆëŠ” ì˜í™”ë§Œ ë§¤í•‘ì— ì¶”ê°€
        original_train_movie_count = len(self.movie2idx)
        
        for movie in self.movies:
            movie_id = movie['_id']
            # í‰ì ì´ ìˆê³ , ì•„ì§ ë§¤í•‘ì— ì—†ëŠ” ì˜í™”ë§Œ ì¶”ê°€
            if movie_id in all_rated_movies and movie_id not in self.movie2idx:
                # ì „ì²˜ë¦¬ì—ì„œ ì œì™¸ë˜ì—ˆì§€ë§Œ í‰ì ì€ ìˆëŠ” ì˜í™”
                new_idx = len(self.movie2idx)
                self.movie2idx[movie_id] = new_idx
                self.idx2movie[new_idx] = movie_id
                all_movies_for_eval[movie_id] = new_idx
        
        print(f"\nâœ“ Total movies (rated movies only): {len(self.movie2idx):,}")
        print(f"  - Training movies (Rating â‰¥ {min_rating_threshold}): {original_train_movie_count:,}")
        print(f"  - Evaluation-only movies (Rating < {min_rating_threshold}): {len(all_movies_for_eval):,}")
        
        # min_rating_threshold ì €ì¥ (ë‚˜ì¤‘ì— ì°¸ê³ ìš©)
        self.min_rating_threshold = min_rating_threshold
        
        # Mappings ì €ì¥
        mappings = {
            'user2idx': self.user2idx,
            'idx2user': self.idx2user,
            'movie2idx': self.movie2idx,
            'idx2movie': self.idx2movie,
            'genre2idx': self.genre2idx,
            'all_genres': self.all_genres,
            'num_users': len(self.user2idx),
            'num_movies': len(self.movie2idx),  # ì „ì²´ ì˜í™” ìˆ˜ (í‰ê°€ìš© í¬í•¨)
            'num_movies_train': len(self.movie2idx) - len(all_movies_for_eval),  # í•™ìŠµìš©ë§Œ
            'num_genres': len(self.all_genres),
            # ì¶”ê°€: inference.pyì—ì„œ ì‚¬ìš©í•˜ëŠ” í‚¤ ì´ë¦„
            'user_idx_to_id': self.idx2user,
            'movie_idx_to_id': self.idx2movie,
            'movie_id_to_idx': self.movie2idx,
        }
        
        with open(self.output_dir / 'mappings.pkl', 'wb') as f:
            pickle.dump(mappings, f)
        
        # Movie features ì €ì¥
        np.save(self.output_dir / 'movie_features.npy', self.movie_features)
        
        # Movie dict ì €ì¥ (LLM í•„í„°ë§ìš©)
        movie_info = {}
        for idx in range(len(self.movie2idx)):
            movie_id = self.idx2movie[idx]
            movie = self.movie_dict.get(movie_id, {})
            
            # ì¥ë¥´ íŒŒì‹±
            genres = []
            genres_str = movie.get('genres', [])
            if genres_str:
                try:
                    genres = eval(', '.join(genres_str))
                except:
                    pass
            
            # ì¶œì—°ì§„ ì´ë¦„
            actors = []
            for actor_id in movie.get('main_cast_people_ids', [])[:5]:
                person = self.peoples.get(actor_id, {})
                name = person.get('korean') or person.get('original', '')
                if name:
                    actors.append(name)
            
            movie_info[movie_id] = {  # movie_idë¥¼ í‚¤ë¡œ ì‚¬ìš©
                'movie_id': movie_id,
                'movie_idx': idx,
                'title': movie.get('title', ''),
                'title_eng': movie.get('title_eng', ''),
                'year': movie.get('year'),
                'grade': movie.get('grade', 'ì •ë³´ ì—†ìŒ'),
                'genres': genres,
                'actors': actors
            }
        
        with open(self.output_dir / 'movie_info.pkl', 'wb') as f:
            pickle.dump(movie_info, f)
        
        print(f"âœ“ All data saved to {self.output_dir}")
        
        # í†µê³„ ì¶œë ¥
        print("\n" + "=" * 50)
        print("Data Statistics")
        print("=" * 50)
        print(f"Users: {len(self.user2idx):,}")
        print(f"Movies: {len(self.movie2idx):,}")
        print(f"Genres: {len(self.all_genres)}")
        print(f"Train ratings: {len(train_df):,}")
        print(f"Valid ratings: {len(valid_df):,}")
        print(f"Test ratings: {len(test_df):,}")
        print(f"Sparsity: {100 * len(train_df) / (len(self.user2idx) * len(self.movie2idx)):.4f}%")


def main():
    """ì „ì²´ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
    
    preprocessor = DataPreprocessor()
    
    # 1. ë°ì´í„° ë¡œë“œ
    preprocessor.load_all_data()
    
    # 2. í¬ì†Œ ë°ì´í„° í•„í„°ë§ ë° í‰ì  threshold ì ìš©
    # Implicit Feedback: Rating 6-10 (ë” ë§ì€ ë°ì´í„°ë¡œ ì„±ëŠ¥ í–¥ìƒ)
    min_rating_threshold = 6  # âœ… 6: ì¤‘ê°„ ì´ìƒ í‰ì  (6-10, 50% ë” ë§ì€ ë°ì´í„°!)
    df = preprocessor.filter_data(
        min_user_interactions=5,   # 5: ë” ë§ì€ ì‚¬ìš©ì í¬í•¨
        min_movie_interactions=3,  # 3: ë” ë§ì€ ì˜í™” í¬í•¨
        min_rating_threshold=min_rating_threshold
    )
    
    # 3. ID ë§¤í•‘ ìƒì„±
    df = preprocessor.create_mappings(df)
    
    # 4. ì˜í™” íŠ¹ì„± ì¶”ì¶œ
    preprocessor.extract_movie_features()
    
    # 5. Train/Valid/Test ë¶„ë¦¬
    train_df, valid_df, test_df = preprocessor.split_data(df)
    
    # 6. ì €ì¥
    preprocessor.save_processed_data(train_df, valid_df, test_df, min_rating_threshold)
    
    print("\n" + "=" * 70)
    print("âœ… ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ (ë…¼ë¬¸ ë°©ì‹)")
    print("=" * 70)
    print("\në…¼ë¬¸ ì •ë ¬ ì„¤ì •:")
    print("  âœ“ Implicit Threshold: Rating â‰¥ 7 (ë…¼ë¬¸: score > 2ì˜ 1-10 ì²™ë„ ë²„ì „)")
    print("  âœ“ Split: Leave-One-Out (ë…¼ë¬¸ê³¼ ë™ì¼)")
    print("  âœ“ Evaluation: ê° ì‚¬ìš©ìì˜ ë§ˆì§€ë§‰ ìƒí˜¸ì‘ìš© ì˜ˆì¸¡")
    print("  âœ“ Temporal Order: ì‹œê°„ ìˆœì„œ ë³´ì¥")
    print("\nì£¼ìš” íŠ¹ì§•:")
    print("  â€¢ Rating 7-10: ë†’ì€ í’ˆì§ˆì˜ positive feedback")
    print("  â€¢ Leave-One-Out: ì‹¤ì œ ì¶”ì²œ í™˜ê²½ê³¼ ë™ì¼í•œ í‰ê°€")
    print("  â€¢ ì‹œê°„ìˆœ ì •ë ¬: ë¯¸ë˜ ì˜ˆì¸¡ task (ë” ì–´ë ¤ì›€)")
    print("\nì˜ˆìƒ ì„±ëŠ¥ (Leave-One-Outì€ Randomë³´ë‹¤ ì–´ë ¤ì›€):")
    print("  â€¢ ëª©í‘œ NDCG@10: > 0.4")
    print("  â€¢ ì°¸ê³ : ë…¼ë¬¸ MovieLens 1m = 0.6+ (ë” ì‰¬ìš´ ë°ì´í„°ì…‹)")


if __name__ == "__main__":
    main()

