# ì˜í™” ì¶”ì²œ ì‹œìŠ¤í…œ

**ëª©í‘œ**: NDCG@10 > 0.4  
**í™˜ê²½**: Google Colab A100 GPU  
**ë°ì´í„°**: NDJSON í˜•ì‹ (movies, peoples, ratings)

---

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
deeplearning/
â”œâ”€â”€ âš™ï¸ ë°ì´í„° ì „ì²˜ë¦¬
â”‚   â”œâ”€â”€ 1_data_preprocessing.py              # ë”¥ëŸ¬ë‹ ì¶”ì²œ ëª¨ë¸ìš© (LightGCN, BPR-MF)
â”‚   â””â”€â”€ 1_data_preprocessing_sequential.py   # Sequential ëª¨ë¸ìš© (BERT4Rec)
â”‚
â”œâ”€â”€ ğŸ§  ë”¥ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ
â”‚   â”œâ”€â”€ 2_train_lightgcn.py     # Graph Neural Network ê¸°ë°˜
â”‚   â”œâ”€â”€ 2_train_bpr_mf.py       # Neural Matrix Factorization
â”‚   â””â”€â”€ 2_train_bert4rec.py     # Transformer ê¸°ë°˜
â”‚
â””â”€â”€ ğŸ¯ ì¶”ë¡ 
    â””â”€â”€ 4_inference.py          # Top-50 ì¶”ì²œ ìƒì„±
```

---

## ğŸš€ ì‹¤í–‰ ë°©ë²•

### Step 1: ë°ì´í„° ì „ì²˜ë¦¬

#### ë°©ë²• A: Graph/Matrix ê¸°ë°˜ ë”¥ëŸ¬ë‹ ëª¨ë¸ (LightGCN, BPR-MF)

**ì²˜ë¦¬ ê³¼ì •**:
- NDJSON íŒŒì¼ ë¡œë“œ (movies, peoples, ratings)
- User-Item ìƒí˜¸ì‘ìš© ê·¸ë˜í”„ ìƒì„±
- Rating ê¸°ë°˜ Positive/Negative ë¶„ë¦¬
- Train/Valid/Test ë¶„í•  (8:1:1)
- COO í˜•ì‹ sparse matrix ìƒì„±

**ì¶œë ¥ íŒŒì¼** (`data/processed/`):
- `mappings.pkl`: User/Movie ID ë§¤í•‘
- `train_interactions.pkl`: í•™ìŠµ ë°ì´í„°
- `valid_interactions.pkl`: ê²€ì¦ ë°ì´í„°
- `test_interactions.pkl`: í…ŒìŠ¤íŠ¸ ë°ì´í„°
- `movie_info.pkl`: ì˜í™” ë©”íƒ€ë°ì´í„°

---

#### ë°©ë²• B: Sequential ëª¨ë¸ (BERT4Rec)

**ì²˜ë¦¬ ê³¼ì •**:
- ì‹œê°„ ìˆœì„œ ë³´ì¡´ (timestamp ê¸°ì¤€ ì •ë ¬)
- Userë³„ ì‹œí€€ìŠ¤ ìƒì„±
- Temporal split (Leave-one-out)
- ìµœì†Œ ìƒí˜¸ì‘ìš© ìˆ˜ í•„í„°ë§ (min_interactions â‰¥ 5)

**ì¶œë ¥ íŒŒì¼** (`data/processed_sequential/`):
- `mappings.pkl`: User/Movie ID ë§¤í•‘
- `train_sequences.pkl`: Userë³„ í•™ìŠµ ì‹œí€€ìŠ¤
- `valid_sequences.pkl`: ê²€ì¦ ë°ì´í„°
- `test_sequences.pkl`: í…ŒìŠ¤íŠ¸ ë°ì´í„°

---

### Step 2: ëª¨ë¸ í•™ìŠµ

#### ëª¨ë¸ 1: LightGCN

**ëª¨ë¸ íŠ¹ì§•**:
- Graph Neural Network (GNN) ê¸°ë°˜ ë”¥ëŸ¬ë‹ ì¶”ì²œ ëª¨ë¸
- User-Item ìƒí˜¸ì‘ìš© ê·¸ë˜í”„ì—ì„œ embedding ì „íŒŒ
- í˜‘ì—… í•„í„°ë§ì„ GNNìœ¼ë¡œ êµ¬í˜„
- He et al., SIGIR 2020

**ì£¼ìš” í•˜ì´í¼íŒŒë¼ë¯¸í„°**:
```python
embedding_dim = 64      # ì„ë² ë”© ì°¨ì›
num_layers = 3          # GNN ë ˆì´ì–´ ìˆ˜
lr = 0.001             # Learning rate
batch_size = 16384      # ë°°ì¹˜ í¬ê¸°
reg_weight = 1e-4       # L2 ì •ê·œí™”
```

**í•™ìŠµ ì‹œê°„**: 15-20ë¶„ (A100)  
**ì˜ˆìƒ NDCG@10**: 0.20-0.40

---

#### ëª¨ë¸ 2: BPR Matrix Factorization (Neural MF)

**ëª¨ë¸ íŠ¹ì§•**:
- PyTorchë¡œ êµ¬í˜„í•œ Neural Matrix Factorization
- Bayesian Personalized Ranking (BPR) Loss ì‚¬ìš©
- ê°„ë‹¨í•˜ì§€ë§Œ ê°•ë ¥í•œ ë”¥ëŸ¬ë‹ baseline

**ì£¼ìš” í•˜ì´í¼íŒŒë¼ë¯¸í„°**:
```python
embedding_dim = 64      # ì„ë² ë”© ì°¨ì›
lr = 0.001             # Learning rate
batch_size = 16384      # ë°°ì¹˜ í¬ê¸°
reg_weight = 1e-4       # L2 ì •ê·œí™”
```

**í•™ìŠµ ì‹œê°„**: 10-15ë¶„ (A100)  
**ì˜ˆìƒ NDCG@10**: 0.15-0.35

---

#### ëª¨ë¸ 3: BERT4Rec (Sequential)

**ëª¨ë¸ íŠ¹ì§•**:
- Transformer ê¸°ë°˜ Sequential ì¶”ì²œ
- Bidirectional self-attention
- Masked item prediction
- Sun et al., CIKM 2019

**ì£¼ìš” í•˜ì´í¼íŒŒë¼ë¯¸í„°**:
```python
d_model = 128           # ì„ë² ë”© ì°¨ì›
num_heads = 4           # Attention heads
num_layers = 2          # Transformer layers
max_seq_len = 50        # ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´
mask_prob = 0.15        # Masking í™•ë¥ 
```

**í•™ìŠµ ì‹œê°„**: 20-30ë¶„ (A100)  
**ì˜ˆìƒ NDCG@10**: 0.25-0.45

---

### Step 3: ì¶”ë¡  ë° ì¶”ì²œ ìƒì„±

**ê¸°ëŠ¥**:
- í•™ìŠµëœ ëª¨ë¸ë¡œ Top-50 ì˜í™” ì¶”ì²œ
- Userë³„ ì¶”ì²œ ê²°ê³¼ ìƒì„±
- ì´ë¯¸ í‰ê°€í•œ ì˜í™” ì œì™¸
- ì¶”ì²œ ì´ìœ  ë¶„ì„ (ì„ íƒ)

**ì¶œë ¥ íŒŒì¼**:
- `recommendations_top50.json`: ìµœì¢… ì¶”ì²œ ê²°ê³¼

---

## ğŸ“Š ë°ì´í„° ì „ì²˜ë¦¬ ìƒì„¸

### Graph/Matrix ê¸°ë°˜ ëª¨ë¸ ì „ì²˜ë¦¬ (`1_data_preprocessing.py`)

**ì£¼ìš” ê¸°ëŠ¥**:

1. **ë°ì´í„° ë¡œë“œ**
   - NDJSON íŒŒì¼ì„ chunkingìœ¼ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ë¡œë“œ
   - Movies: ì˜í™” ì •ë³´ (ì œëª©, ì¥ë¥´, ê°ë…, ë°°ìš° ë“±)
   - Ratings: User-Movie-Rating-Timestamp

2. **í•„í„°ë§**
   - ìµœì†Œ ìƒí˜¸ì‘ìš© ìˆ˜ í•„í„°ë§
   - User: min 5ê°œ ì´ìƒ
   - Movie: min 10ê°œ ì´ìƒ

3. **Positive/Negative ì •ì˜**
   - Positive: Rating â‰¥ 7
   - Negative: Rating < 7
   - (ë˜ëŠ” Explicit Negative ì „ëµ ì‚¬ìš© ê°€ëŠ¥)

4. **ë°ì´í„° ë¶„í• **
   - Train: 80%
   - Valid: 10%
   - Test: 10%
   - Random split ë˜ëŠ” Temporal split

5. **ì¶œë ¥ í˜•ì‹**
   - COO (Coordinate) format sparse matrix
   - User ID, Item ID ë§¤í•‘ í…Œì´ë¸”

---

### Sequential ì „ì²˜ë¦¬ (`1_data_preprocessing_sequential.py`)

**ì£¼ìš” ê¸°ëŠ¥**:

1. **ì‹œê°„ ìˆœì„œ ë³´ì¡´**
   - Timestamp ê¸°ì¤€ ì •ë ¬
   - Userë³„ í‰ê°€ ì‹œí€€ìŠ¤ ìƒì„±

2. **ì‹œí€€ìŠ¤ ìƒì„±**
   - ê° Userì˜ ì „ì²´ í‰ê°€ ì´ë ¥ì„ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜
   - ìµœì†Œ ì‹œí€€ìŠ¤ ê¸¸ì´ í•„í„°ë§ (min 5ê°œ)

3. **Temporal Split**
   - Train: ì‹œê°„ìƒ ì²˜ìŒ ~ (n-2)ë²ˆì§¸
   - Valid: (n-1)ë²ˆì§¸
   - Test: në²ˆì§¸ (ê°€ì¥ ìµœê·¼)

4. **ì¶œë ¥ í˜•ì‹**
   - Userë³„ ì‹œí€€ìŠ¤ ë¦¬ìŠ¤íŠ¸
   - Item IDëŠ” 1ë¶€í„° ì‹œì‘ (0ì€ padding/mask)

---

## ğŸ§  ëª¨ë¸ í•™ìŠµ ìƒì„¸

### LightGCN

**ì•Œê³ ë¦¬ì¦˜**:
```
1. Userì™€ Itemì˜ ì´ˆê¸° embeddingì„ Neural Networkë¡œ í•™ìŠµ
2. Graph Convolutionìœ¼ë¡œ embedding ì „íŒŒ
   - Layer 0: ì´ˆê¸° embedding
   - Layer k: ì´ì›ƒ ë…¸ë“œë“¤ì˜ í‰ê·  (Message Passing)
3. ëª¨ë“  ë ˆì´ì–´ì˜ embeddingì„ í‰ê· í•˜ì—¬ ìµœì¢… embedding ìƒì„±
4. BPR Lossë¡œ í•™ìŠµ (Pairwise Ranking)
```

**ì†ì‹¤ í•¨ìˆ˜**:
- BPR Loss: userê°€ positive itemì„ negative itemë³´ë‹¤ ì„ í˜¸í•˜ë„ë¡ í•™ìŠµ
- L2 Regularization: Overfitting ë°©ì§€

**í‰ê°€ ì§€í‘œ**:
- NDCG@10: Top-10 ì¶”ì²œì˜ ìˆœìœ„ í’ˆì§ˆ (ì •ê·œí™”ëœ DCG)
- Recall@10: Top-10ì— ì •ë‹µì´ í¬í•¨ëœ ë¹„ìœ¨
- HR@10: Hit Rate (ì •ë‹µ í¬í•¨ ì—¬ë¶€)

---

### BPR Matrix Factorization (Neural MF)

**ì•Œê³ ë¦¬ì¦˜**:
```
1. User embeddingê³¼ Item embeddingì„ Neural Networkë¡œ í•™ìŠµ
2. User biasì™€ Item bias ì¶”ê°€ (ì„ í˜• ë³´ì •)
3. Score = userÂ·item + user_bias + item_bias + global_bias
4. BPR Lossë¡œ í•™ìŠµ (Pairwise Ranking)
```

**íŠ¹ì§•**:
- Neural Networkë¡œ êµ¬í˜„í•œ Matrix Factorization
- LightGCNë³´ë‹¤ ë‹¨ìˆœí•˜ì§€ë§Œ íš¨ê³¼ì 
- Graph êµ¬ì¡° ì—†ì´ ë‹¨ìˆœ embeddingìœ¼ë¡œ í•™ìŠµ
- ë¹ ë¥¸ í•™ìŠµ ë° ì¶”ë¡ 

---

### BERT4Rec

**ì•Œê³ ë¦¬ì¦˜**:
```
1. Userì˜ ì‹œí€€ìŠ¤ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ìŒ
2. ëœë¤í•˜ê²Œ ì¼ë¶€ itemì„ [MASK]ë¡œ ì¹˜í™˜
3. Transformerë¡œ ì–‘ë°©í–¥ context í•™ìŠµ
4. Masked item ì˜ˆì¸¡
5. Cross Entropy Lossë¡œ í•™ìŠµ
```

**íŠ¹ì§•**:
- ì–‘ë°©í–¥ ëª¨ë¸ë§ (ì´ì „ + ì´í›„ context)
- Sequential íŒ¨í„´ í•™ìŠµ
- Cold-start ë¬¸ì œì— ê°•í•¨

---

## ğŸ¯ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ê°€ì´ë“œ

### LightGCN

```python
# ì„±ëŠ¥ ìš°ì„  (ë©”ëª¨ë¦¬ ì¶©ë¶„)
embedding_dim = 128
num_layers = 3
batch_size = 16384
lr = 0.001

# ì†ë„ ìš°ì„  (ë©”ëª¨ë¦¬ ë¶€ì¡±)
embedding_dim = 64
num_layers = 2
batch_size = 32768
lr = 0.001

# Overfitting ë°œìƒ ì‹œ
reg_weight = 5e-4    # 1e-4 â†’ 5e-4
patience = 3         # Early stopping
lr = 0.0005          # Learning rate ê°ì†Œ
```

---

### BPR-MF

```python
# ê¸°ë³¸ ì„¤ì •
embedding_dim = 64
lr = 0.001
batch_size = 16384
reg_weight = 1e-4

# ì„±ëŠ¥ ê°œì„ 
embedding_dim = 128  # ë” í° ì„ë² ë”©
lr = 0.0005         # ë” ì‘ì€ learning rate
```

---

### BERT4Rec

```python
# ê¸°ë³¸ ì„¤ì •
d_model = 128
num_heads = 4
num_layers = 2
max_seq_len = 50
mask_prob = 0.15

# ì„±ëŠ¥ ìš°ì„ 
d_model = 256        # ë” í° ëª¨ë¸
num_layers = 4       # ë” ê¹Šì€ ëª¨ë¸

# ì†ë„ ìš°ì„ 
d_model = 64         # ë” ì‘ì€ ëª¨ë¸
num_layers = 1       # ì–•ì€ ëª¨ë¸
batch_size = 512     # í° ë°°ì¹˜
```

---

## ğŸ“ˆ ì„±ê³µ ê¸°ì¤€

```
âœ… NDCG@10 â‰¥ 0.40: Excellent
âœ… NDCG@10 â‰¥ 0.30: Good
âš ï¸  NDCG@10 â‰¥ 0.20: Acceptable
âŒ NDCG@10 < 0.20: Need Improvement
```

**ìµœì¢… ìˆ˜ì •**: 2025-11-04  
**ë²„ì „**: v2.0
